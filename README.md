# Fine-tune-Serve-a-Small-Language-Model

A comprehensive project for fine-tuning and serving small language models using modern ML tools and best practices.

## ğŸš€ Quick Start

### 1. Setup Python Environment

```bash
# Create virtual environment
python -m venv .venv

# Activate virtual environment
# Windows (PowerShell):
.\.venv\Scripts\Activate.ps1
# Windows (Command Prompt):
.\.venv\Scripts\activate.bat
# Linux/Mac:
source .venv/bin/activate

# Install dependencies
pip install -r requirements.txt
```

### 2. Setup Hugging Face Token

```bash
# Run the interactive setup script
python scripts/setup_huggingface.py
```

This will:
- Guide you through getting a Hugging Face token
- Test the token validity
- Save it to `.env` file
- Configure cache directories

### 3. Test Configuration

```bash
# Test Hugging Face setup
python scripts/test_huggingface.py
```

## ğŸ“ Project Structure

```
Fine-tune-Serve-a-Small-Language-Model/
â”œâ”€â”€ config/                    # Configuration files
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ huggingface_config.py  # HF token management
â”œâ”€â”€ data/                      # Raw and processed data
â”œâ”€â”€ src/                       # Source code
â”‚   â”œâ”€â”€ data_collection/       # Data collection scripts
â”‚   â”œâ”€â”€ data_processing/       # Data preprocessing and cleaning
â”‚   â”œâ”€â”€ dataset_generation/    # Dataset creation and formatting
â”‚   â”œâ”€â”€ training/              # Model training scripts
â”‚   â”œâ”€â”€ evaluation/            # Model evaluation and metrics
â”‚   â”œâ”€â”€ deployment/            # Model serving and deployment
â”‚   â””â”€â”€ orchestration/         # Workflow orchestration
â”œâ”€â”€ notebooks/                 # Jupyter notebooks for exploration
â”œâ”€â”€ scripts/                   # Utility scripts
â”‚   â”œâ”€â”€ setup_huggingface.py   # HF token setup
â”‚   â””â”€â”€ test_huggingface.py    # Configuration testing
â”œâ”€â”€ .env                       # Environment variables (auto-generated)
â”œâ”€â”€ .gitignore                 # Git ignore rules
â”œâ”€â”€ requirements.txt           # Python dependencies
â””â”€â”€ README.md                  # This file
```

## ğŸ”§ Configuration

### Hugging Face Token

The project uses Hugging Face tokens for:
- Downloading pre-trained models
- Uploading fine-tuned models
- Accessing datasets
- Using gated models

To get your token:
1. Go to [Hugging Face Settings](https://huggingface.co/settings/tokens)
2. Click "New token"
3. Give it a name (e.g., "fine-tuning-project")
4. Select "Write" role for full access
5. Copy the generated token

The token is automatically saved to `.env` file and configured in the environment.

### Environment Variables

Key environment variables:
- `HUGGINGFACE_TOKEN`: Your Hugging Face API token
- `HF_TOKEN`: Alternative token variable
- `HF_HOME`: Cache directory for models
- `TRANSFORMERS_CACHE`: Cache directory for transformers

## ğŸ› ï¸ Key Tools

- **LangChain**: Framework for LLM applications
- **Hugging Face**: Model hub and transformers library
- **PyPDF2**: PDF processing
- **BeautifulSoup**: Web scraping
- **FastAPI**: API framework for model serving
- **DVC**: Data version control
- **Weights & Biases**: Experiment tracking
- **LoRA**: Low-rank adaptation for efficient fine-tuning

## ğŸ”„ CI/CD

GitHub Actions workflow (`.github/workflows/ci.yml`) includes:
- Python environment setup
- Dependency installation
- Code linting and formatting
- Basic testing
- Security checks

## ğŸ“š Usage Examples

### Download a Model
```python
from transformers import AutoModel, AutoTokenizer

# Download model (requires token for gated models)
model = AutoModel.from_pretrained("microsoft/DialoGPT-medium")
tokenizer = AutoTokenizer.from_pretrained("microsoft/DialoGPT-medium")
```

### Load a Dataset
```python
from datasets import load_dataset

# Load dataset
dataset = load_dataset("squad", split="train")
```

### Use Configuration
```python
from config.huggingface_config import hf_config

# Check if logged in
if hf_config.is_logged_in():
    print("Ready to use Hugging Face!")
```

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests if applicable
5. Submit a pull request

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.